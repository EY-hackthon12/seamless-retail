<div align="center">

# ğŸ§  Cognitive Retail Brain

### Agentic AI for Seamless Omnichannel Retail

[![Status](https://img.shields.io/badge/Status-Production_Ready-brightgreen)](.)
[![Stack](https://img.shields.io/badge/Stack-FastAPI%20|%20React%20|%20LangGraph%20|%20Docker-blue)](.)
[![CLaRa](https://img.shields.io/badge/RAG-CLaRa_Powered-orange)](https://arxiv.org/abs/2511.18659)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

*A hyperscale, multi-agent AI system that bridges the "context gap" in omnichannel retail, powered by CLaRa's state-of-the-art document compression and adaptive hardware scaling.*

[Quick Start](#-quick-start) â€¢ [Architecture](#-architecture) â€¢ [LLM Hosting](#-llm-hosting) â€¢ [Training](#-training) â€¢ [Docs](#-documentation)

</div>

---

## ğŸ“– Overview

The **Cognitive Retail Brain** is an advanced multi-agent AI system designed to solve the "context gap" in omnichannel retail. It enables seamless customer journeys by maintaining conversation history and preferences across different touchpoints (Mobile App â†’ In-Store Kiosk).

### Key Innovations

| Feature | Description |
|---------|-------------|
| ğŸ¤– **Multi-Agent Orchestration** | Hub-and-spoke architecture with GPT-4o Router and specialized sub-agents |
| ğŸ”— **Seamless Context Handover** | QR-based session transfer from Mobile to Kiosk |
| ğŸ§ª **CLaRa RAG Integration** | 32-64x document compression with unified retrieval-generation |
| âš¡ **Adaptive LLM Hosting** | Auto-scales from CPU to datacenter GPUs |
| ğŸ§  **Hardware-Aware Scaling** | "Brain power" increases with GPU resources |

---

## âœ¨ Features

### Core Capabilities
- **Multi-Agent Architecture**: GPT-4o Router orchestrates Sales, Inventory, Recommendation, and Loyalty agents
- **Persistent Memory**: PostgreSQL stores sessions, cart items, and conversation history
- **Local Model Hosting**: Fine-tuned Mistral-7B, StarCoder2, and NLLB-200 models
- **Modern UI**: React + TypeScript + Tailwind CSS for Mobile and Kiosk interfaces

### CLaRa-Powered RAG
- **Document Compression**: 32x-64x compression while preserving semantic content
- **Three-Stage Training**: Compression Pretraining â†’ Instruction Tuning â†’ End-to-End
- **Unified Optimization**: Joint retrieval and generation in shared latent space
- **Citation Grounding**: Automatic source attribution for responses

### Adaptive Hardware Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HARDWARE TIER SCALING                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tier         â”‚ VRAM     â”‚ Backend     â”‚ Batch     â”‚ Context        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU Only     â”‚ N/A      â”‚ llama.cpp   â”‚ 1         â”‚ 2,048          â”‚
â”‚ Low VRAM     â”‚ <8GB     â”‚ llama.cpp   â”‚ 1         â”‚ 4,096          â”‚
â”‚ Consumer     â”‚ 8-12GB   â”‚ vLLM        â”‚ 8         â”‚ 8,192          â”‚
â”‚ Prosumer     â”‚ 12-24GB  â”‚ vLLM        â”‚ 32        â”‚ 16,384         â”‚
â”‚ Datacenter   â”‚ 24GB+    â”‚ vLLM        â”‚ 128       â”‚ 32,768         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- OpenAI API Key (for GPT-4o Router)
- NVIDIA GPU (optional, for local model hosting)

### Installation

```bash
# 1. Clone the repository
git clone <repo-url>
cd seamless-retail

# 2. Configure environment
cp .env.example .env
# Edit .env with your API keys:
#   OPENAI_API_KEY=sk-your-key-here
#   POSTGRES_USER=postgres
#   POSTGRES_PASSWORD=password
#   POSTGRES_DB=retail_brain

# 3. Run with Docker
docker-compose up --build

# 4. Access the application
#    Mobile App: http://localhost:8000
#    Kiosk: http://localhost:8000/kiosk
#    API Docs: http://localhost:8000/docs
```

### Local Model Hosting (Optional)

```bash
# Detect hardware and configure automatically
python -m cognitive_brain.core.hardware_detector

# Start optimized LLM server
python scripts/hosting/serve_optimized.py --port 8001

# Or use the batch script (Windows)
scripts/hosting/run_host_optimized.bat
```

---

## ğŸ—ï¸ Architecture

### System Overview

```mermaid
graph TB
    subgraph "Frontend"
        MA[Mobile App]
        KI[Kiosk Interface]
    end
    
    subgraph "API Gateway"
        FE[FastAPI Backend]
    end
    
    subgraph "Agent Orchestration"
        RT[GPT-4o Router]
        SA[Sales Agent]
        IA[Inventory Agent]
        RA[Recommendation Agent]
        LA[Loyalty Agent]
    end
    
    subgraph "Cognitive Brain"
        HD[Hardware Detector]
        AE[Adaptive Engine]
        RM[RAG Master + CLaRa]
    end
    
    subgraph "Data Layer"
        PG[(PostgreSQL)]
        VD[(Vector DB)]
    end
    
    MA --> FE
    KI --> FE
    FE --> RT
    RT --> SA
    RT --> IA
    RT --> RA
    RT --> LA
    SA --> AE
    RA --> RM
    RM --> VD
    FE --> PG
```

### CLaRa RAG Pipeline

```mermaid
graph LR
    subgraph "Three-Stage Training"
        S1[Stage 1: Compression Pretraining]
        S2[Stage 2: Instruction Tuning]
        S3[Stage 3: End-to-End]
    end
    
    subgraph "Inference"
        D[Documents] --> C[Compress 32x]
        C --> R[Rerank Top-K]
        R --> G[Generate + Cite]
    end
    
    S1 --> S2 --> S3
    S3 -.-> C
```

### Agent Workflow

1. **Mobile**: User asks "Do you have a blue suit?" â†’ Sales Agent â†’ Inventory Agent â†’ Response
2. **Handover**: User opens App â†’ Generates QR (Session ID)
3. **Kiosk**: User scans QR â†’ Context retrieved â†’ Recommendation Agent suggests matching shoes

---

## ğŸ–¥ï¸ LLM Hosting

The Cognitive Brain automatically detects hardware and configures optimal inference:

### Supported Backends

| Backend | Use Case | Requirements |
|---------|----------|--------------|
| **vLLM** | High-throughput serving | CUDA, â‰¥8GB VRAM |
| **llama.cpp** | CPU/low VRAM inference | Any hardware |
| **PyTorch** | Custom models, fine-tuning | GPU recommended |
| **Triton** | Vision models (YOLO) | NVIDIA GPU |

### Hardware Detection

```python
from cognitive_brain.core.hardware_detector import HardwareDetector

detector = HardwareDetector()
detector.print_summary()

# Output:
# ============================================================
#   COGNITIVE BRAIN - HARDWARE DETECTION REPORT
# ============================================================
#   Hardware Tier: CONSUMER
#   Backend: vllm
#   Quantization: awq
#   Max Batch Size: 8
#   BRAIN POWER SCORE: ğŸ§ ğŸ§  80 (Consumer)
# ============================================================
```

### Model Registry

| Model | Purpose | VRAM (AWQ) | VRAM (FP16) |
|-------|---------|------------|-------------|
| Mistral-7B-Instruct | Language & Reasoning | 4.5 GB | 14 GB |
| StarCoder2-3B | Code Generation | 2.0 GB | 6 GB |
| NLLB-200 | Translation | 3.0 GB | 8 GB |
| CLaRa-7B | RAG Compression | 4.5 GB | 14 GB |

ğŸ“š **Full Guide**: [docs/LLM_HOSTING_GUIDE.md](docs/LLM_HOSTING_GUIDE.md)

---

## ğŸ“ Training

### Supported Fine-Tuning

| Agent | Base Model | Method | Context |
|-------|------------|--------|---------|
| Language & Reasoning | Mistral-7B | QLoRA | 8192 |
| Code Generation | StarCoder2-3B | QLoRA | 8192 |
| Translation | NLLB-200 | Full Fine-tune | 512 |
| RAG Compression | Mistral-7B | CLaRa 3-Stage | 2048 |

### Quick Training

```bash
# Code Agent (StarCoder2-3B)
python scripts/training/train_code_agent.py \
    --model_name bigcode/starcoder2-3b \
    --output_dir trained_models/code_agent

# Translation Agent (NLLB-200)
python scripts/training/train_nllb_translation.py \
    --model_name facebook/nllb-200-distilled-600M

# CLaRa Training (3 Stages)
cd ml-clara-main
bash scripts/train_pretraining.sh          # Stage 1
bash scripts/train_instruction_tuning.sh   # Stage 2
bash scripts/train_stage_end_to_end.sh     # Stage 3
```

ğŸ“š **Full Guide**: [docs/TRAINING_GUIDE.md](docs/TRAINING_GUIDE.md)

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [Project Overview](docs/project_overview.md) | Architecture and design philosophy |
| [User Guide](docs/user_guide.md) | Step-by-step demo walkthrough |
| [LLM Hosting Guide](docs/LLM_HOSTING_GUIDE.md) | Complete hosting documentation |
| [CLaRa Integration](docs/CLARA_INTEGRATION.md) | RAG compression architecture |
| [Hardware Requirements](docs/HARDWARE_REQUIREMENTS.md) | Scaling from CPU to datacenter |
| [Training Guide](docs/TRAINING_GUIDE.md) | Model fine-tuning specifications |
| [Training Specs](docs/training_specs/) | Per-agent hyperparameters |

---

## ğŸ“‚ Directory Structure

```
seamless-retail/
â”œâ”€â”€ agents/                 # LangGraph agent definitions
â”œâ”€â”€ app/                    # FastAPI application
â”‚   â”œâ”€â”€ api/                # REST endpoints
â”‚   â”œâ”€â”€ core/               # Configuration
â”‚   â”œâ”€â”€ db/                 # Database schema
â”‚   â””â”€â”€ services/           # Business logic
â”œâ”€â”€ cognitive_brain/        # Adaptive inference engine
â”‚   â”œâ”€â”€ core/               # Hardware detection
â”‚   â”œâ”€â”€ inference/          # vLLM, llama.cpp, PyTorch backends
â”‚   â””â”€â”€ orchestration/      # Model orchestration
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ frontend/               # React application
â”œâ”€â”€ infra/                  # Docker, Helm, K8s configs
â”œâ”€â”€ ml-clara-main/          # CLaRa RAG implementation
â”œâ”€â”€ models/                 # Local LLM weights
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ hosting/            # LLM serving scripts
â”‚   â””â”€â”€ training/           # Fine-tuning scripts
â””â”€â”€ trained_models/         # Fine-tuned adapters
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technologies |
|-------|--------------|
| **Backend** | FastAPI, LangGraph, LangChain, SQLAlchemy |
| **Frontend** | React, Vite, TypeScript, Tailwind CSS |
| **Database** | PostgreSQL, Vector DB (planned) |
| **AI/ML** | OpenAI GPT-4o, Mistral, StarCoder2, CLaRa |
| **Inference** | vLLM, llama.cpp, PyTorch, Triton |
| **Infrastructure** | Docker, Docker Compose, Kubernetes |

---

## ğŸ§ª Verification

Run these commands to verify your setup:

```bash
# Hardware detection
python -m cognitive_brain.core.hardware_detector

# Adaptive engine test
python -m cognitive_brain.inference.adaptive_engine

# Full verification
python scripts/verify_llm_hosting.py
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please read the documentation and follow the code style.

1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **CLaRa**: [Apple ML Research](https://arxiv.org/abs/2511.18659) for the document compression framework
- **OpenRLHF**: Training framework foundation
- **vLLM**: High-throughput LLM serving
- **LangGraph**: Agent orchestration

---

<div align="center">

**Built with ğŸ§  for the EY Techathon 6.0**

</div>
