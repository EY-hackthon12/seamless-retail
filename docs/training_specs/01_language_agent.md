# Language & Reasoning Agent Training Spec

## Model Details
- **Model**: Mistral 7B Instruct v0.3 (or latest)
- **Objective**: General instruction following, reasoning, and persona alignment.
- **Training Method**: QLoRA (4-bit quantization).

## Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **Learning Rate (LR)** | 2.0e-4 | Standard starting LR for QLoRA to ensure effective adaptation without instability. |
| **Scheduler** | Cosine with Warmup | Smooths convergence; prevents early divergence. |
| **Warmup Ratio** | 0.03 (3%) | Gradually ramps up LR to stabilize training early on. |
| **Epochs** | 2 - 3 | Sufficient for instruction tuning; >3 often leads to overfitting on small datasets. |
| **Batch Size** | 32 (Effective) | Achieved via Gradient Accumulation (e.g., 4 per GPU × 8 accumulation steps). |
| **Context Length** | 4096 - 8192 | Mistral supports sliding window up to 16k, but 8k is optimal for VRAM/speed balance. |
| **LoRA Rank (r)** | 64 | High enough to capture complex reasoning patterns. |
| **LoRA Alpha** | 128 | Standard practice is Alpha = 2 × Rank for stability. |
| **Target Modules** | All Linear | `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`. Targeting all yields better performance. |
| **Weight Decay** | 0.01 | Regularization to prevent overfitting. |

## Data & Preprocessing
- **Format**: ChatML (User/Assistant structure).
- **Datasets**: Alpaca (Cleaned), ShareGPT, OpenHermes, and Self-Instruct data generated by the Router (GPT-4).

### Prompt Template
```
<|im_start|>system
You are a helpful, intelligent assistant...<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
{response}<|im_end|>
```
