# ==============================================================================
# Optimized GPU Dockerfile for Extreme Speed LLM Hosting
# ==============================================================================
# CLaRA-Inspired Optimizations Applied:
# 1. Multi-stage build for minimal final image
# 2. vLLM with PagedAttention (5x throughput)
# 3. TF32/BF16 precision hints
# 4. Optimal CUDA memory allocation
# 5. Layer caching for fast rebuilds
# ==============================================================================

FROM pytorch/pytorch:2.1.2-cuda12.1-cudnn8-runtime AS base

LABEL maintainer="Seamless Retail AI Team"
LABEL description="GPU-optimized LLM hosting with CLaRA optimizations"
LABEL version="2.0.0"

WORKDIR /app

# ==============================================================================
# SYSTEM DEPENDENCIES (Cached Layer)
# ==============================================================================
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    git \
    ninja-build \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# ==============================================================================
# PYTHON DEPENDENCIES (Cached Layer)
# ==============================================================================

# Install vLLM first (largest dependency, cached separately)
# PagedAttention provides 2-4x throughput improvement over HF generate
RUN pip install --no-cache-dir vllm==0.3.0

# Install FAISS GPU for accelerated retrieval
RUN pip install --no-cache-dir faiss-gpu

# Install app dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ==============================================================================
# APPLICATION CODE
# ==============================================================================
COPY . .

# ==============================================================================
# ENVIRONMENT VARIABLES - SPEED OPTIMIZATIONS
# ==============================================================================

# Python optimization
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# vLLM configuration
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# CUDA/PyTorch optimizations
# TF32 for Ampere+ GPUs (3x matmul speedup)
ENV TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
# Expandable memory segments (prevents fragmentation)
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
# NCCL timeout for distributed training
ENV NCCL_TIMEOUT=5400

# Application defaults
ENV BASE_MODEL=bigcode/starcoder2-3b
ENV QUANTIZATION=4bit
ENV MAX_BATCH_SIZE=8
ENV LOBE_TIMEOUT_SECONDS=5.0

# ==============================================================================
# HEALTH CHECK & ENTRYPOINT
# ==============================================================================
EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "scripts/hosting/serve_optimized.py", "--port", "8000"]
